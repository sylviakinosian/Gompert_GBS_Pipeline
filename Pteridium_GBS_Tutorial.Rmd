---
title: "*Pteridium* GBS Pipeline"
author: "Sylvia Kinosian"
output: html_document
---
This pipeline was designed by Zach Gompert and edited by Martin Schilling and Sylvia Kinosian. All of the associated files can be found at: https://github.com/sylviakinosian/Pteridium_GBS_Pipeline 

The following protocol was designed to process .FASTQ files from ddRADseq data for 4 species of the fern genus *Pteridium*: *P. aquilinum* (diploid), *P. esculentum* (diploid), *P. semihaustatum* (tetraploid), and *P. caudatum* (tetraploid). There is no reference genome for *Pteridium*, so we create a reference *de novo* using the diploid species only. When calling variants, we chose to use the GATK HaplotypeCaller because it allows the user to specify ploidy. We clustered the diploids and tetraploids separately (each run specifying ), then recombined the resulting SNPs (additive, not averaged).

Further analyses are done using the program ENTROPY to estimate admixture among the individuals. The goal of this analysis is to determine the parentage of the tetraploid species, which are hypothesized to be allotetraploids. We are also interested in the admixture within the diploid species, with regards to revising taxonomic rank.

There are three main steps to the GBS Pipeline (parsing, building the reference, and calling variants); running ENTROPY also has roughly three steps (seeding values with DAPC, running ENTROPY, visualizing and further analysis in R).

#{.tabset}

##GBS Pipeline

###**1) Parse barcodes and split .FASTQ by individual**
This first step uses perl scripts to parse the barcodes from the raw GBS .FASTQ data and then split that raw file into indiviual .FASTQ files. Note that all Perl were run using Perl 5 (http://www.perl.org).

####a. 
The first scipt, parse\_barcodes768.pl, requires two files: a barcodes file and the raw GBS.FASTQ file. The barcodes.txt file has three columns: the index name, barcode, and sample name for each indiviual. See barcodes.txt for an example. 

Usage:
```{bash eval=FALSE}
parse_barcodes768.pl barcodes.txt Pter_01_S17_L002_R1_001.fastq
```

####b. 
The next script, splitFastq\_ms.pl, requires two files: a list of individuals and the parsed.GBS.FASTQ file (one of the output file from parse\_barcodes768.pl). The individuals.txt file is simply a list of the names for each individual you would like to create a .FASTQ file for (a text file contain one column, with one name per row, no header).

Usage:
```{bash eval=FALSE}
splitFastq_ms.pl individuals.txt parsed_Pter_01_S17_L002_R1_001.fastq
```
Now, you should have a .FASTQ file for each individual listed in the indivduals.txt file. 

###**2) Building our reference**

### PLEASE NOTE: this is for the diploid species ONLY (*P. aquilinum* and *P. esculentum*). 

Now that we have extracted the raw information for each individual, it's time to build our reference genome. There is no reference genome available for *Pteridium*, so we are going to build one *de novo* using the two diploid species. Since the tetraploids are most likely allotetraploids (of hybrid origin) we are only using the diploids because that will capture most of the sequence variation and is cleaner than dealing with the possibly divergent sequences on the tetraploid hybrids.

We decided to build the reference genome by clustering similar sequences within species, and then combining those sequences across individuals. For the final reference, we made sure that each contig was represented in BOTH diploid species.

**Before starting**, transfer the .FASTQ files for the diploid individuals to separate folders. This will make the clustering within each species much easier.

Below is a command to copy a list of files into different folder. The file\_list.txt is a list of the file names (name.fastq) for the individuals you would like to move.

```{bash eval=FALSE}
xargs cp -t /path/to/folder < file_list.txt
```

###Step 1: Cluster highly similar sequences in individual .FASTQ files

####a.
The first part of this step is to convert the .FASTQ files into .FASTA files using the program SEQTK (v. 1.2-r102-dirty). See the script seqtk.sh (below) for an example of how to loop through the files in a directory.

```{bash eval=FALSE}
for i in *.fastq; do
    id=$(echo $i | cut -f1 -d.)
    echo $id
	seqtk seq -a $i > $id.fasta
	done
```

####b.
The second part is to use the program VSEARCH (v. 2.4.2) to cluster sequences **within each individual** with a 98% similarity to create "centroids". 

usage of vsearch.sh:

```{bash eval=FALSE}
vsearch.sh 0.98 centroids
```

VSEARCH command "under the hood" of vsearch.sh:
```{bash eval=FALSE}
vsearch --cluster_fast indivdual.fasta --id 0.98 --iddef 2 --threads 8 --centroids centroids/centroids98_individual.fasta
```

###Step 2: combine centroids from preceeding runs and cluster at 92% similarity

####a. *P. aquilinum*
```{bash eval=FALSE}
cat *.fasta aqui_consensus.fasta

vsearch --cluster_fast aqui_consensus.fasta --threads 10 --iddef 2 --id 0.92 --consout 92aquiCons.fasta --msaout 92aquiMsa.fasta
```

####b. *P. esculentum*
```{bash eval=FALSE}
cat *.fasta escu_consensus.fasta

vsearch --cluster_fast consensus_escu.fasta --threads 10 --iddef 2 --id 0.92 --consout cons_escu92.fasta --msaout msa_escu92.fasta
```

###Step 3: Cluster at 84% similarity, using the file clustered at 92% similarity as the "reference". Then remove paralogs

####a. *P. aquilinum* - cluster at 84% similarity, using the file clustered at 92% similarity as the "reference"

```{bash eval=FALSE}
vsearch --cluster_fast 92aquiCons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84aquiCons.fasta --msaout 84aquiMsa.fasta
```

####b. *P. esculentum* - cluster at 84% similarity, using the file clustered at 92% similarity as the "reference"

```{bash eval=FALSE}
vsearch --cluster_fast 92escuCons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84escuCons.fasta --msaout 84escuMsa.fasta
```

####c. Remove collapsed clusters (paralogs) from files clustered at 84% similarity

The remove\_collapsed\_clusters.py script removes all entries that have (the 2nd) seqs > 1.

```{bash eval=FALSE}
./remove_collapsed_clusters.py 84aquiCons.fasta RCCaqui.fasta
sta

902952 uncollapsed clusters found
```

```{bash eval=FALSE}
./remove_collapsed_clusters.py 84escuCons.fasta RCCescu.fasta

444932 uncollapsed clusters found
```

The resulting RCC\*.fasta files will be used in step 5

###Step 4: combine aqui and escu, re-run vsearch clustering and filter

####a. BEFORE COMBINING
Make sure your *aquilinum* and *esculentum* inds are marked separatley within the RCC\*.fasta files. This will make checking to see if each final contig is represented by each species much easier.

```{bash eval=FALSE}
sed 's/^>centroid=centroid=/centroid=centroid=a/g' RCCaqui.fasta > aRCCaqui.fasta

sed 's/^>centroid=centroid=/centroid=centroid=e/g' RCCescu.fasta > eRCCescu.fasta
```
####b. Combine *aquilinum* and *esculentum*

```{bash eval=FALSE}
cat aRCCaqui.fasta eRCCescu.fasta > ae_cons.fasta
```

####c. Re-run vsearch with an id (% similarity) of your choice (92,88,86,84...)

This clustering step in done to ensure that the individual contigs isolated are present in both *aquilinum* and *esculentum*.

```{bash eval=FALSE}
# cluster at 88% similarity
vsearch --cluster_fast ae_cons.fasta --threads 10 --iddef 2 --id 0.88 --consout 88ae_consout.fasta --msaout 88ae_msaout.fasta

# remove all clusters that are present in only one species
./presence_filter.pl 88ae_msaout.fasta

finished, retained 85422 contigs
```

```{bash eval=FALSE}
# cluster at 84% similarity
vsearch --cluster_fast ae_cons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84ae_consout.fasta --msaout 84ae_msaout.fasta

# remove all clusters that are present in only one species
./presence_filter.pl 84ae_msaout.fasta

finished, retained 105428 contigs
```

We chose to use the 84% similarity. The proided us with a large number of contigs present in both species. We are going to apply some much stricter filtering parameters later on in the variant calling step, so it is better to start off with a few more contigs / material in general.

Hooray! Now you have a fresh *de novo* reference with which to align your parsed .FASTQ files!

###**3) Alignment of parsed reads and variant calling**

###Step 1: Prepare the reference sequence

####a. Index the reference (consensus) sequence. 

Here, we used the Burrow-Wheeler Aligner (BWA v. 0.7.10) to index our reference genome. This give the squence position points for the alignment later on.

```{bash eval=FALSE}
bwa index ae_consensus_final.fasta 
```

####b. Picard tools to create a dictionary

We used Java (OpenJDK) v. 1.8.0 and PicardTools v. 2.9.0

```{bash eval=FALSE}
java -jar picard.jar CreateSequenceDictionary REFERENCE=ae_consensus_final.fasta OUTPUT=ae_consensus_final.dict
```
####c. Creating the fasta index file

We used SAMTOOLS v. 1.5

```{bash eval=FALSE}
samtools faidx ae_consensus_final.fasta
```

###Step 2: Align parsed reads (from ALL individual .FASTQ files) to the *de novo* reference

####a. Align individuals with BWA ALN

See script `bwa_aln.sh`

```{bash eval=FALSE}
#! /bin/bash

REF='/uufs/chpc.utah.edu/common/home/wolf-group2/skinosian/3pteridium/parse/fastq/ae_consensus_final.fasta'

for i in *.fastq;
do
ids=$(echo $i | cut -f1 -d.)
echo $ids

/uufs/chpc.utah.edu/common/home/u6009816/apps/bwa-0.7.15/bwa aln -n 4 -l 20 -k 2 -t 8 -q 10 -f $ids.sai $REF $i

/uufs/chpc.utah.edu/common/home/u6009816/apps/bwa-0.7.15/bwa samse -n 1 -r "@RG\tID:$ids\tLB:$ids\tSM:$ids\tPL:ILLUMINA" -f $ids.sam $REF $ids.sai $i

done
```

The output is .SAM files for each individual

####b. Convert files from .SAM to .BAM, sort, and index the individuals using SAMTOOLS

```{bash eval=FALSE}
samtools view -o *.bam *.sam

samtools sort -o *.sorted.bam *.bam
 
samtools index -b *.sorted.bam
```

Because there were about 100 individuals, we used a fork manager to run this through the University of Utah Center for High Performance Computing cluster (CHPC). The result is a .bam, .sorted.bam, and .sorted.bam.bai file for each individual. See the script fork\_view\_sort\_index.pl for an example.

###Step 3: Call Variants

To call variants, we used the GATK HaplotypeCaller (v. 3.8.0) because of its ability to specify ploidy. We called variants separately for the diploids and tetraploids.

####Diploids
```{bash eval=FALSE}
java -Xmx48g -jar GenomeAnaysisTK.jar -T HaplotypeCaller -R ae_consensus_final -I diploid_bams.list --genotyping_mode DISCOVERY -ploidy 2 -o 2ae_rawVar.vcf -out_mode EMIT_VARIANTS_ONLY
```

####Tetraploids
```{bash eval=FALSE}
java -Xmx48g -jar GenomeAnaysisTK.jar -T HaplotypeCaller -R ae_consensus_final -I tetraploid_bams.list --genotyping_mode DISCOVERY -ploidy 4 -o 4ae_rawVar.g.vcf -out_mode EMIT_VARIANTS_ONLY --variant_index_type LINEAR --variant_index_parameter 128000
```

```{bash eval=FALSE}
grep -v ^# ae.vcf | cut -f 8 | perl -p -i -e 's/DP=(\d+);\S+/\1/' > depth.txt
```

###Step 4: filter VCFs

VCFTOOLS (v. 0.1.15) can be used to filter diploids (see below), but because it does not support tetraploids we created a custom Python script to filter instead.

```{bash eval=FALSE}
vcftools --remove-filtered-all --remove-indels --maf 0.1 --max-maf 0.99 --min-meanDP 2.0 --max-missing 0.3 --minQ 20 --recode-INFO-all --recode --vcf 2ae\_rawVar.vcf
```

The script vcfFilter.py filters based on read depth (minCoverage), alternative alleles(minAltRds), fixed loci (notFixed), and mapping quality (mapQual). These variables can be altered within the file to achieve the desired filtering affect (see **stringency variable** in script below). 

BOTH VCF files need to be filtered this way (2ae\_rawVar.vcf and 4ae\_rawVar.vcf).

```{python eval=FALSE}
#! /usr/bin/python
# 
# This script filters a vcf file based on overall sequence coverage, number of
# non-reference reads, number of alleles, and reverse orientation reads.  
# See below for default values, and to change them, if necessary. Additionally,
# note that currently, the number of retained loci is being written at the end
# of the file. 
# Usage: ./vcfFilter.py <variant file>.vcf > outfile.vcf

import sys
import re
import shutil
#import tempfile

# stringency variables, edit as desired
minCoverage = 20 # minimum number of seqs; DP
minAltRds = 4 # minimum number of sequences with the alternative allele; AC
notFixed = 1.0 # removes loci fixed for alt; AF
mapQual = 30 # minimum mapping quality


n_seqs_retained = int()
with open(sys.argv[1], 'rb') as file:
    for line in file:
        if line[0] == '#':
            print line.split('\n')[0]
        else:
            dp = int(re.findall('DP=[0-9]+', line)[0].split('=')[1])
            ac = int(re.findall('AC=[0-9]+', line)[0].split('=')[1])
            af = float(re.findall('AF=[0.0-9.0]+', line)[0].split('=')[1]) 
	    if re.findall('MQ=NaN', line):
                continue # some of the MQ are NaN, let's just skip those (they would be filtered out anyways)
            else: 
                mq = float(re.findall('MQ=[0.0-9.0]+', line)[0].split('=')[1])
                if (dp >= minCoverage and ac >= minAltRds and af != notFixed and mq >= mapQual):
                    print line.split('\n')[0]
                    n_seqs_retained += 1
                else:
                    continue
    file.close()

print '#Retained %i variable loci' % n_seqs_retained
```

###Step 5: Find the intersection of variants in diploids and tetraploids

Since we now have two VCF files, we need to combine them again somehow. To do this, we find the **intersection** of the variants in both files, subset, and then re-combine.

####a. Intersection of variants

```{bash eval=FALSE}
perl vcf_checker.pl filtered_ae2.vcf filtered_4ae.vcf
```

Output is a list of the contigs present in both VCF files called matches.txt

####b. Subset VCF files with list of intersection matches

Do this for both files.

```{bash eval=FALSE}
perl subsetVcf.pl matches.vcf filtered_ae2.vcf

perl subsetVcf.pl matches.vcf filtered_ae4.vcf
```

Output is two files: sub\_filtered\_ae2.vcf and sub\_filtered\_ae4.vcf

####c. Combine files

What we are doing here is appending the data from one file onto the end of each matching contig in the other file.

```{bash eval=FALSE}
perl combine.pl sub\_filtered\_ae2.vcf sub\_filtered\_4ae.vcf
```

This outputs a file called aeAll.vcf

**Add CHROM lines from each to aeAll.vcf**

##Estimation of Admixture

###**Entropy**

Before getting started with Entopry, we need to convert our VCF file to a GL (Genotype Likelihood) file.

We used the perl script vcf2gl.pl to convert our filtered vcf to the simpler .gl format for downstream analysis.

For diploids:

```{bash eval=FALSE}
perl vcf2gl.pl aeAll.vcf
```
This outputs a file called out.recode.gl

Next we are going to convert the GL file to a matrix that we can use in R with DAPC.

```{bash eval=FALSE}
perl gl2genest.pl out.recode.gl
```
This outputs a file called pntest\_out.recode.gl

###Discriminant Analysis of Principle Components 

Among the diploid species, *Pteridium aquilinum* and *P. esculentum*, and the tetraploid species, *P. semihaustatum* and *P. caudatum*, there are 15 sub-species. These distinctions are based mostly on morphology, and so testing the population structure among them will help distinguish the validity of these biological species and sub-species.

#####a. seed entropy with values from DAPC

Using the R package ADEGENET (v. 2.1.1), run a Discriminate Analysis of Principle Componets (DAPC function) to seed values in ENTROPY so we don't get label swapping. We followed the [DAPC vignette](adegenet.r-forge.r-project.org/files/tutorial-dapc.pdf).

```{r eval=FALSE}
library(adegenet)

# read in genotype matrix
d <- read.table("pntest_out.recode.vcf", header = F)

# transform data
dt <- t(d)

# convert to genind object
dg <- df2genind(dt, sep = " ", ploidy = 2)

grp <- find.clusters(dg, max.n.clust = 15)
# number of PCs retained: 60
# number of clusters: 2

head(grp$grp, 97)

# get likelihood assignments

dapc1 <- dapc(dg, grp$grp)
# PCs 60
# discriminant dunctions: 1

write.table(dapc1$posterior, "k_est.txt")
```

#####b. run ENTROPY

```{bash eval=FALSE}
./entropy -b 2000 -t 4 -k 2 -i ae_in.gl -o out.hdf5 -m 1 -w 0 -q pop_ests.txt -s 20
```

#####c. ESTPOST - pulling out meaningful things from entropy

```{bash eval=FALSE}
/home/skinosian/hts_tools/estpost_h5_entropy -o out_d -p deviance -s 3 -w 1 entropy_ae_k2_2.hdf5
```

###**Visualizing Admixture**

```{r eval=FALSE}
# function to plot each chain for a given k
plot_q_per_chain <- function(kqlist, xlabel, kval, ...){
	cols <- c('#A8FFFD', '#BB61C3', '#a39d9d', 'yellow', '#FF7373', '#FFFAC1', '#7FFF00', '#26CDCD', '#E4D5EF', '#CDBA8F', '#B862D3', '#79D958', '#CA4B87', '#D2C948', '#6386CA', '#D1543B', '#0D0D41')
	par(mfrow= c(length(kqlist),1), mar=c(4,2,1,1) + 0.1, oma= c(5,0,0,0), mgp= c(0,1,0))
	chain <- seq(1, length(kqlist), 1) 
	for(i in 1:length(kqlist)){
		barplot(t(kqlist[[i]]), beside= F, col= cols, las= 2, axisnames= T, cex.name= 1, cex.axis= 0.5, border= 1, space= c(0.05), yaxt= 'n', cex.lab= 2, names.arg= xlabel, main = paste("k =", kval, sep = ' '), cex.main= 1.5)
		axis(2, at= c(0, 0.5, 1), cex.axis= 1, las= 2, pos= -0.2)
 	}
}

# read in files from estpost
k2_1 <- read.csv("k2_1.txt", sep = ',', header = T)
k2_2 <- read.csv("k2_2.txt", sep = ',', header = T)
k2_3 <- read.csv("k2_3.txt", sep = ',', header = T)

k2_all <- as.data.frame(matrix(nrow = 291, ncol = 4))
k2_all[,1] <- k2_1[,1]
k2_all[,2] <- k2_2[,1]
k2_all[,3] <- k2_3[,1]

# average together the three chains
k2_all[,4] <- rowMeans(k2_all[sapply(k2_all, is.numeric)])

k2_df <- as.data.frame(matrix(nrow = 97, ncol = 3))
k2_df[,1] <- k2_all[1:97,4]
k2_df[,2] <- k2_all[98:194,4]

# read in names
names <- read.csv("names.txt", header = F)

k2_df[,3] <- names[,2]

k2_ordered <- k2_df[order(k2_df$V1),]
k2_list <- list(k2_ordered[,1:2])

oNames <- k2_ordered[,3]

plot_q_per_chain(k2_list, oNames, 2)

# repeat for other ks

#please <- names[match(correct_names[,1], names[,1]),]

make_data_frame <- function(k_file, n_inds, k_val, ...){
	df <- as.data.frame(matrix(nrow = n_inds, ncol = k_val + 1))
	j = 1
	for (i in 1:k_val){
		df[,i] <- k_file[j:(n_inds+1),1]
		j = j + n_inds
	}
	return(k_file)
}

```

##Program Versions

Below is a list of all program versions used in this analysis. Please note that newer versions of these software packages *may* work for this pipeline, but be aware that usage often changes with new verions. 

[Perl 5](https://www.perl.org/)

[Python 2.7.13](https://www.python.org/downloads/release/python-2713/)

[SAMtools v. 1.5](https://sourceforge.net/projects/samtools/files/samtools/1.5/)

[SEQTK 1.2-r102-dirty](https://github.com/lh3/seqtk)

[VSEARCH 2.4.2](https://github.com/torognes/vsearch)

[BWA 0.7.15](https://sourceforge.net/projects/bio-bwa/files/)

[PicardTools 2.9.0](https://github.com/broadinstitute/picard/releases)

[GATK v.3.8.0](https://software.broadinstitute.org/gatk/download/archive) - [HaplotypeCaller](https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php)

ENTROPY (coming soon)

ESPOST (coming soon)
