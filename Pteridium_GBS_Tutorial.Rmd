---
title: "*Pteridium* GBS Pipeline"
author: "Sylvia Kinosian"
output: html_document
---
This pipeline was designed by Zach Gompert and edited by Martin Schilling and Sylvia Kinosian. All of the associated filed can be found at: https://github.com/sylviakinosian/Pteridium_GBS_Pipeline 

It is designed to process GBS data from .FASTQ files from ddRADseq data for 4 species of the fern genus *Pteridium*: *P. aquilinum* (diploid), *P. esculentum* (diploid), *P. semihaustatum* (tetraploid), and *P. caudatum* (tetraploid). There is no reference genome for *Pteridium*, so we create a reference *de novo* using the diploid species. When calling variants, we chose to use the GATK HaplotypeCaller because it allows the user to specify ploidy. We clustered the diploids and tetraploids separately (each run specifying ), then recombined the resulting SNPs (additive, not averaged).

Further analyses will be done using the program ENTROPY to estimate admixture among the individuals. The goal of this analysis is to determine the parentage of the tetraploid species, which are hypothesized to be allotetraploids.

There are three main steps to the GBS Pipeline (parsing, building the reference, and calling variants); running ENTROPY also has roughly three steps (seeding values with DAPC, running ENTROPY, visualizing and further analysis in R).
 
##Parse barcodes and split .FASTQ by individual
This first step uses perl scripts to parse the barcodes from the raw GBS data and then split that raw file into indiviual .FASTQ files.

####a. 
The first scipt, parse_barcodes768.pl, requires two files: a barcodes file and the raw GBS.FASTQ file. The barcodes.txt file has three columns: the index name, barcode, and sample name for each indiviual. See barcodes.txt for an example. 

Usage:
```{bash eval=FALSE}
parse_barcodes768.pl barcodes.txt Pter_01_S17_L002_R1_001.fastq
```

####b. 
The next scipt, splitFastq_ms.pl, requires two files: a list of individuals and the parsed.GBS.FASTQ file (one of the output file from parse_barcodes768.pl). The individuals.txt file is simply a list of the names for each individual you would like to create a .FASTQ file for (a text file contain one column, with one name per row, no header).

Usage:
```{bash eval=FALSE}
splitFastq_ms.pl individuals.txt parsed_Pter_01_S17_L002_R1_001.fastq
```
Now, you should have a .FASTQ file for each individual listed in the indivduals.txt file. 

##Building our reference

### PLEASE NOTE: this is for the diploid species ONLY (*P. aquilinum* and *P. esculentum*). 

Now that we have extracted the raw information for each individual, it's time to build our reference genome. There is no reference genome available for *Pteridium*, so we are going to build one *de novo* using the two diploid species. We are not using the tetraploid species, because of their ploidy levels and hybrid status.

We decided to build the reference genome by clustering similar sequences within species, and then combining those sequences across individuals. For the final reference, we made sure that each contig was represented in BOTH diploid species.

**Before starting**, transfer the .FASTQ files for the diploid individuals to separate folders. This will make the clustering within each species much easier.

Below is a command to copy a list of files into different folder. The file_list.txt is a list of the file names (name.fastq) for the individuals you would like to move.

```{bash eval=FALSE}
xargs cp -t /path/to/folder < file_list.txt
```

###Step 1: Cluster highly similar sequences in individual .FASTQ files

####a. 
The first part of this step is to convert the .fastq files into .fasta files using the program SEQTK. See the script seqtk.sh for an example of how to loop through the files in a directory.

####b.
The second part is to use the program VSEARCH to cluster sequences **within each individual** with a 98% similarity to create "centroids". 

```{bash eval=FALSE}
vsearch.sh 0.98 centroids
```

###Step 2: combine centroids from preceeding runs and cluster at 92% similarity

####a. *P. aquilinum*
```{bash eval=FALSE}
cat *.fasta aqui_consensus.fasta

vsearch --cluster_fast aqui_consensus.fasta --threads 10 --iddef 2 --id 0.92 --consout 92aquiCons.fasta --msaout 92aquiMsa.fasta
```

####b. *P. esculentum*
```{bash eval=FALSE}
cat *.fasta escu_consensus.fasta

vsearch --cluster_fast consensus_escu.fasta --threads 10 --iddef 2 --id 0.92 --consout cons_escu92.fasta --msaout msa_escu92.fasta
```

###Step 3: cluster the resulting cluster resulting species consensus at 84% similarity, using the file clustered at 92% similarity as the "reference"; remove paralogs

####a. *P. aquilinum* - cluster at 84% similarity, using the file clustered at 92% similarity as the "reference"

```{bash eval=FALSE}
vsearch --cluster_fast 92aquiCons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84aquiCons.fasta --msaout 84aquiMsa.fasta
```

####b. *P. esculentum* - cluster at 84% similarity, using the file clustered at 92% similarity as the "reference"

```{bash eval=FALSE}
vsearch --cluster_fast 92escuCons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84escuCons.fasta --msaout 84escuMsa.fasta
```

####c. Remove collapsed clusters (paralogs) from files clustered at 84% similarity

The remove_collapsed_clusters.py script removes all entries that have (the 2nd) seqs > 1.

```{bash eval=FALSE}
./remove_collapsed_clusters.py 84aquiCons.fasta RCCaqui.fasta
sta

902952 uncollapsed clusters found
```

```{bash eval=FALSE}
./remove_collapsed_clusters.py 84escuCons.fasta RCCescu.fasta

444932 uncollapsed clusters found
```

The resulting RCC\*.fasta files will be used in step 5

###Step 4: combine aqui and escu, re-run vsearch clustering and filter

####a. BEFORE COMBINING
Make sure your *aquilinum* and *esculentum* inds are marked separatley within the RCC\*.fasta files. This will make checking to see if each final contig is represented by each species much easier.

```{bash eval=FALSE}
sed 's/^>centroid=centroid=/centroid=centroid=a/g' RCCaqui.fasta > aRCCaqui.fasta

sed 's/^>centroid=centroid=/centroid=centroid=e/g' RCCescu.fasta > eRCCescu.fasta
```
####b. Combine *aquilinum* and *esculentum*

```{bash eval=FALSE}
cat aRCCaqui.fasta eRCCescu.fasta > ae_cons.fasta
```

####c. Re-run vsearch with an id (% similarity) of your choice (92,88,86,84...)

This clustering step in done to ensure that the individual contigs isolated are present in both *aquilinum* and *esculentum*.

```{bash eval=FALSE}
# cluster at 88% similarity
vsearch --cluster_fast ae_cons.fasta --threads 10 --iddef 2 --id 0.88 --consout 88ae_consout.fasta --msaout 88ae_msaout.fasta

# remove all clusters that are present in only one species
./presence_filter.pl 88ae_msaout.fasta

finished, retained 85422 contigs
```

```{bash eval=FALSE}
# cluster at 84% similarity
vsearch --cluster_fast ae_cons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84ae_consout.fasta --msaout 84ae_msaout.fasta

# remove all clusters that are present in only one species
./presence_filter.pl 84ae_msaout.fasta

finished, retained 105428 contigs
```

We chose to use the 84% similarity. The proided us with a large number of contigs present in both species. We are going to apply some much stricter filtering parameters later on in the variant calling step, so it is better to start off with a few more contigs / material in general.

Hooray! Now you have a fresh *de novo* reference with which to align your parsed .FASTQ files!

##Alignment of parsed reads and variant calling

###Step 1: Prepare the reference sequence

####a. Index the reference (consensus) sequence. 

Here, we used the Burrow-Wheeler Aligner (BWA v. 0.7.10) to index our reference genome. This give the squence position points for the alignment later on.

```{bash eval=FALSE}
bwa index ae_consensus_final.fasta 
```

####b. Picard tools to create a dictionary

```{bash eval=FALSE}
java -jar picard.jar CreateSequenceDictionary REFERENCE=ae_consensus_final.fasta OUTPUT=ae_consensus_final.dict
```
####c. Creating the fasta index file

We used SAMTOOLS v. 1.5

```{bash eval=FALSE}
samtools faidx ae_consensus_final.fasta
```

###Step 2: Align parsed reads (from ALL individual .FASTQ files) to the *de novo* reference

####a. Align individuals with BWA MEM

```{bash eval=FALSE}
#!/bin/bash
# this script goes through all *.fastq files and aligns to a consensus file; splits out .sam files

# edit the directory (DIR) to your the directory containing all .fastq files
# edit the "cut -f*" in line 9 to match the section of path the cut

DIR="/uufs/chpc.utah.edu/common/home/wolf-group2/skinosian/2pteridium/inds/"

for i in $DIR*.fastq; do
    file=$(echo $i | cut -f10 -d/)
    ids=$(echo $file | cut -f1 -d.)
    echo $ids
    /uufs/chpc.utah.edu/common/home/u6009816/apps/bwa-0.7.15/bwa mem -t 10 -w 50 -k 20 -a -C -R "@RG\tID:$ids\tLB:$ids\tSM:$ids\tPL:ILLUMINA" ae_consensus_final.fasta $i > out/${ids}.sam

done
```

output is .SAM files for each individual

####b. Convert files from .SAM to .BAM, sort, and index the individuals using SAMTOOLS

```{bash eval=FALSE}
samtools view -o *.bam *.sam

samtools sort -o *.sorted.bam *.bam
 
samtools index -b *.sorted.bam
```

Because there were about 100 individuals, we used a fork manager to run this through the University of Utah Center for High Performance Computing cluster (CHPC). The result is a .bam, .sorted.bam, and .sorted.bam.bai file for each individual.

```{perl eval=FALSE}
#!/usr/bin/perl

use warnings;
use strict;
use Parallel::ForkManager;

my $max = shift(@ARGV); ## get number of cores to use at one time

my $pm = Parallel::ForkManager->new($max);

FILES:
foreach my $file (@ARGV){ ## loop through set of fastq files
        $pm->start and next FILES;
        my $base = $file;
        $base =~ s/sam// or die "failed to remove sam name\n";
        $base =~ s!.*/!! or die "failed to remove path\n";
        # system commands
		#
		# these are the important SAMTOOLS commands:
		system "samtools view -o $base"."bam $file\n";
        system "samtools sort -o $base"."sorted.bam $base"."bam\n";
        system "samtools index -b $base"."sorted.bam\n";
		#
        $pm->finish; ## exit the child process
}
$pm->wait_all_children;
```

###Step 3: Call Variants

To call variants, we used the GATK HaplotypeCaller (v. 3.8.0) because of its ability to specify ploidy. We called variants separately for the diploids and tetraploids.

####Diploids
```{bash eval=FALSE}
java -Xmx48g -jar GenomeAnaysisTK.jar -T HaplotypeCaller -R ae_consensus_final -I diploid_bams.list --genotyping_mode DISCOVERY -ploidy 2 -o 2ae_rawVar.g.vcf -out_mode EMIT_VARIANTS_ONLY --variant_index_type LINEAR --variant_index_parameter 128000
```

####Tetraploids
```{bash eval=FALSE}
java -Xmx48g -jar GenomeAnaysisTK.jar -T HaplotypeCaller -R ae_consensus_final -I tetraploid_bams.list --genotyping_mode DISCOVERY -ploidy 4 -o 4ae_rawVar.g.vcf -out_mode EMIT_VARIANTS_ONLY --variant_index_type LINEAR --variant_index_parameter 128000
```

```{bash eval=FALSE}
grep -v ^# ae.vcf | cut -f 8 | perl -p -i -e 's/DP=(\d+);\S+/\1/' > depth.txt
```

After calling variants, we used the program VCFTOOLS to filter the resulting .vcf file.

##Estimation of Admixture - still to be done :(

###Entropy

Among the diploid species, *Pteridium aquilinum* and *P. esculentum*, and the tetraploid species, *P. semihaustatum* and *P. caudatum*, there are 15 sub-species. These distinctions are based mostly on morphology, and so testing the population structure among them will help distinguish the validity of these biological species and sub-species.

#####a. seed entropy with values from DAPC

Using the R package ADEGENET, run a Discriminate Analysis of Principle Componets (DAPC function) to seed values in ENTROPY so we don't get label swapping

```{r}
library(adegenet)

```

#####b. run ENTROPY

```{bash eval=FALSE}
./entropy -b 2000 -t 4 -k 2 -i ae_in.gl -o out.hdf5 -m 1 -w 0
```

#####c. estpost - pulling out meaningful things from entropy

```{bash eval=FALSE}
/home/skinosian/hts_tools/estpost_h5_entropy -o out_d -p deviance -s 3 -w 1 entropy_ae_k2_2.hdf5
```

